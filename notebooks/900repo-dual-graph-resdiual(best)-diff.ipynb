{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:26:51.075799Z",
     "iopub.status.busy": "2025-10-30T02:26:51.075465Z",
     "iopub.status.idle": "2025-10-30T02:26:57.227220Z",
     "shell.execute_reply": "2025-10-30T02:26:57.226237Z",
     "shell.execute_reply.started": "2025-10-30T02:26:51.075772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from reinforcement_model.baseline import SimpleClassifier\n",
    "import torch.optim as optim\n",
    "# from reinforcement_model.ppo import PPOSegmentClassifier\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "# from reinforcement_model.dataset import CommitDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List, Any\n",
    "from tree_sitter import Parser, Node\n",
    "from pandarallel import pandarallel\n",
    "import multiprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "multiprocessing.cpu_count()\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8)\n",
    "# import sys\n",
    "# sys.setrecursionlimit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:26:57.228716Z",
     "iopub.status.busy": "2025-10-30T02:26:57.228306Z",
     "iopub.status.idle": "2025-10-30T02:27:00.527823Z",
     "shell.execute_reply": "2025-10-30T02:27:00.526669Z",
     "shell.execute_reply.started": "2025-10-30T02:26:57.228691Z"
    }
   },
   "outputs": [],
   "source": [
    "neg = pd.read_csv(r'./datasets/negative+CC-900repos.csv')\n",
    "neg['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:00.528837Z",
     "iopub.status.busy": "2025-10-30T02:27:00.528636Z",
     "iopub.status.idle": "2025-10-30T02:27:01.414193Z",
     "shell.execute_reply": "2025-10-30T02:27:01.413338Z",
     "shell.execute_reply.started": "2025-10-30T02:27:00.528816Z"
    }
   },
   "outputs": [],
   "source": [
    "pos = pd.read_csv(r'./datasets/positive+CC-900repos.csv', encoding='utf_8_sig')\n",
    "pos['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:01.415725Z",
     "iopub.status.busy": "2025-10-30T02:27:01.415523Z",
     "iopub.status.idle": "2025-10-30T02:27:01.458663Z",
     "shell.execute_reply": "2025-10-30T02:27:01.457598Z",
     "shell.execute_reply.started": "2025-10-30T02:27:01.415707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>github</th>\n",
       "      <th>message</th>\n",
       "      <th>diff</th>\n",
       "      <th>label</th>\n",
       "      <th>project_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/osclass/OSClass/commit/ff7e...</td>\n",
       "      <td>OSClass 2.3.5</td>\n",
       "      <td>diff --git a/CHANGELOG.txt b/CHANGELOG.txt\\nin...</td>\n",
       "      <td>1</td>\n",
       "      <td>osclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/gosa-project/gosa-core/comm...</td>\n",
       "      <td>escape html entities to fix xss at the login s...</td>\n",
       "      <td>diff --git a/html/index.php b/html/index.php\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>gosa-project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/dgl/cgiirc/commit/dd8d50752...</td>\n",
       "      <td>0.5.11</td>\n",
       "      <td>diff --git a/client-perl.cgi b/client-perl.cgi...</td>\n",
       "      <td>0</td>\n",
       "      <td>dgl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/GNOME/libxml2/commit/b215c2...</td>\n",
       "      <td>Fix cleanup of attributes in XML reader\\n\\nxml...</td>\n",
       "      <td>diff --git a/xmlreader.c b/xmlreader.c\\nindex ...</td>\n",
       "      <td>0</td>\n",
       "      <td>GNOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/apache/camel/commit/6b979d0...</td>\n",
       "      <td>CAMEL-10575: snakeyaml: add an option to filte...</td>\n",
       "      <td>diff --git a/components/camel-snakeyaml/src/ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>apache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10107</th>\n",
       "      <td>https://github.com/viabtc/viabtc_exchange_serv...</td>\n",
       "      <td>Update README.md</td>\n",
       "      <td>diff --git a/README.md b/README.md\\nindex 52f6...</td>\n",
       "      <td>0</td>\n",
       "      <td>viabtc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>https://github.com/qinxuye/cola/commit/d791a1c...</td>\n",
       "      <td>fix counter error; cluster/stage will check st...</td>\n",
       "      <td>diff --git a/cola/cluster/master.py b/cola/clu...</td>\n",
       "      <td>0</td>\n",
       "      <td>qinxuye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>https://github.com/open-classifieds/openclassi...</td>\n",
       "      <td>Merge pull request #3146 from oliverds/master\\...</td>\n",
       "      <td>diff --git a/oc/classes/image.php b/oc/classes...</td>\n",
       "      <td>0</td>\n",
       "      <td>open-classifieds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10110</th>\n",
       "      <td>https://github.com/opnsense/core/commit/2573b7...</td>\n",
       "      <td>firmware: do not show subscription key on firm...</td>\n",
       "      <td>diff --git a/src/opnsense/scripts/firmware/pro...</td>\n",
       "      <td>0</td>\n",
       "      <td>opnsense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>https://github.com/FFmpeg/FFmpeg/commit/00e818...</td>\n",
       "      <td>avcodec/ac3_parser: Check init_get_bits8() for...</td>\n",
       "      <td>diff --git a/libavcodec/ac3_parser.c b/libavco...</td>\n",
       "      <td>1</td>\n",
       "      <td>FFmpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10112 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  github  \\\n",
       "0      https://github.com/osclass/OSClass/commit/ff7e...   \n",
       "1      https://github.com/gosa-project/gosa-core/comm...   \n",
       "2      https://github.com/dgl/cgiirc/commit/dd8d50752...   \n",
       "3      https://github.com/GNOME/libxml2/commit/b215c2...   \n",
       "4      https://github.com/apache/camel/commit/6b979d0...   \n",
       "...                                                  ...   \n",
       "10107  https://github.com/viabtc/viabtc_exchange_serv...   \n",
       "10108  https://github.com/qinxuye/cola/commit/d791a1c...   \n",
       "10109  https://github.com/open-classifieds/openclassi...   \n",
       "10110  https://github.com/opnsense/core/commit/2573b7...   \n",
       "10111  https://github.com/FFmpeg/FFmpeg/commit/00e818...   \n",
       "\n",
       "                                                 message  \\\n",
       "0                                          OSClass 2.3.5   \n",
       "1      escape html entities to fix xss at the login s...   \n",
       "2                                                 0.5.11   \n",
       "3      Fix cleanup of attributes in XML reader\\n\\nxml...   \n",
       "4      CAMEL-10575: snakeyaml: add an option to filte...   \n",
       "...                                                  ...   \n",
       "10107                                   Update README.md   \n",
       "10108  fix counter error; cluster/stage will check st...   \n",
       "10109  Merge pull request #3146 from oliverds/master\\...   \n",
       "10110  firmware: do not show subscription key on firm...   \n",
       "10111  avcodec/ac3_parser: Check init_get_bits8() for...   \n",
       "\n",
       "                                                    diff  label  \\\n",
       "0      diff --git a/CHANGELOG.txt b/CHANGELOG.txt\\nin...      1   \n",
       "1      diff --git a/html/index.php b/html/index.php\\n...      1   \n",
       "2      diff --git a/client-perl.cgi b/client-perl.cgi...      0   \n",
       "3      diff --git a/xmlreader.c b/xmlreader.c\\nindex ...      0   \n",
       "4      diff --git a/components/camel-snakeyaml/src/ma...      1   \n",
       "...                                                  ...    ...   \n",
       "10107  diff --git a/README.md b/README.md\\nindex 52f6...      0   \n",
       "10108  diff --git a/cola/cluster/master.py b/cola/clu...      0   \n",
       "10109  diff --git a/oc/classes/image.php b/oc/classes...      0   \n",
       "10110  diff --git a/src/opnsense/scripts/firmware/pro...      0   \n",
       "10111  diff --git a/libavcodec/ac3_parser.c b/libavco...      1   \n",
       "\n",
       "           project_name  \n",
       "0               osclass  \n",
       "1          gosa-project  \n",
       "2                   dgl  \n",
       "3                 GNOME  \n",
       "4                apache  \n",
       "...                 ...  \n",
       "10107            viabtc  \n",
       "10108           qinxuye  \n",
       "10109  open-classifieds  \n",
       "10110          opnsense  \n",
       "10111            FFmpeg  \n",
       "\n",
       "[10112 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([neg[['github','message','diff','label']],pos[['github','message','diff','label']]],axis=0)\n",
    "df.fillna('', inplace=True)\n",
    "# 1是100%的意思\n",
    "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "label2id={'negative':0,'positive':1}\n",
    "df = shuffled_df.replace({\"label\": label2id})\n",
    "# df = df.sample(1000,random_state=42)\n",
    "df['project_name'] = df['github'].str.extract(r'github\\.com/([^/]+)')\n",
    "# df= df.sample(1000)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:01.459438Z",
     "iopub.status.busy": "2025-10-30T02:27:01.459239Z",
     "iopub.status.idle": "2025-10-30T02:27:01.469325Z",
     "shell.execute_reply": "2025-10-30T02:27:01.468496Z",
     "shell.execute_reply.started": "2025-10-30T02:27:01.459419Z"
    }
   },
   "outputs": [],
   "source": [
    "from unidiff import PatchSet, UnidiffParseError\n",
    "import io\n",
    "def parse_single_diff(diff_text):\n",
    "    parsed_diff = {}\n",
    "\n",
    "    if not isinstance(diff_text, str) or not diff_text.strip():\n",
    "        return parsed_diff\n",
    "\n",
    "    try:\n",
    "        patch = PatchSet(io.StringIO(diff_text))\n",
    "    except UnidiffParseError:\n",
    "        # print(\"Skipping invalid diff:\", diff_text[:200])  # 仅打印前200字符\n",
    "        return parsed_diff\n",
    "\n",
    "    for file in patch:\n",
    "        file_path = file.path\n",
    "        added_lines = []\n",
    "        removed_lines = []\n",
    "\n",
    "        for hunk in file:\n",
    "            for line in hunk:\n",
    "                if line.is_added:\n",
    "                    added_lines.append(line.value.strip())\n",
    "                elif line.is_removed:\n",
    "                    removed_lines.append(line.value.strip())\n",
    "\n",
    "        if file_path not in parsed_diff:\n",
    "            parsed_diff[file_path] = {'added': [], 'removed': []}\n",
    "        parsed_diff[file_path]['added'].extend(added_lines)\n",
    "        parsed_diff[file_path]['removed'].extend(removed_lines)\n",
    "\n",
    "    return parsed_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:01.470216Z",
     "iopub.status.busy": "2025-10-30T02:27:01.470020Z",
     "iopub.status.idle": "2025-10-30T02:27:01.473603Z",
     "shell.execute_reply": "2025-10-30T02:27:01.472800Z",
     "shell.execute_reply.started": "2025-10-30T02:27:01.470198Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['code_diff'] = df['diff'].map(lambda  x : parse_single_diff(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:01.474269Z",
     "iopub.status.busy": "2025-10-30T02:27:01.474100Z",
     "iopub.status.idle": "2025-10-30T02:27:01.479744Z",
     "shell.execute_reply": "2025-10-30T02:27:01.478974Z",
     "shell.execute_reply.started": "2025-10-30T02:27:01.474253Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_before_after_code(diff_text):\n",
    "    parsed_diff = {}\n",
    "\n",
    "    try:\n",
    "        patch = PatchSet(io.StringIO(diff_text))\n",
    "    except UnidiffParseError:\n",
    "        return {}\n",
    "\n",
    "    for file in patch:\n",
    "        file_path = file.path\n",
    "\n",
    "        code_before_lines = []\n",
    "        code_after_lines = []\n",
    "        \n",
    "        for hunk in file:\n",
    "            for line in hunk:\n",
    "                if line.is_added:\n",
    "                    code_after_lines.append(line.value.strip())\n",
    "                elif line.is_removed:\n",
    "                    code_before_lines.append(line.value.strip())\n",
    "                elif not (line.is_added or line.is_removed):  # 上下文行\n",
    "                    code_before_lines.append(line.value.strip())\n",
    "                    code_after_lines.append(line.value.strip())\n",
    "\n",
    "        code_before = '\\n'.join(code_before_lines) if code_before_lines else None\n",
    "        code_after = '\\n'.join(code_after_lines) if code_after_lines else None\n",
    "\n",
    "        parsed_diff[file_path] = (code_before, code_after)\n",
    "\n",
    "    return parsed_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:01.480492Z",
     "iopub.status.busy": "2025-10-30T02:27:01.480322Z",
     "iopub.status.idle": "2025-10-30T02:27:01.483618Z",
     "shell.execute_reply": "2025-10-30T02:27:01.482877Z",
     "shell.execute_reply.started": "2025-10-30T02:27:01.480474Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['before_after'] = df['diff'].map(lambda  x : extract_before_after_code(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:09.957286Z",
     "iopub.status.busy": "2025-10-30T02:27:09.956954Z",
     "iopub.status.idle": "2025-10-30T02:27:09.964736Z",
     "shell.execute_reply": "2025-10-30T02:27:09.963912Z",
     "shell.execute_reply.started": "2025-10-30T02:27:09.957261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from unidiff import PatchSet, UnidiffParseError\n",
    "from tree_sitter import Language, Parser\n",
    "Language.build_library(\n",
    "    'build/my-languages.so',  # 输出的动态链接库\n",
    "    [\n",
    "        'tree-sitter-python',  # Python 语法\n",
    "        'tree-sitter-java',    # Java 语法\n",
    "        'tree-sitter-javascript',  # JavaScript 语法\n",
    "        'tree-sitter-php/php',\n",
    "        'tree-sitter-perl',\n",
    "        'tree-sitter-c',\n",
    "        # 'tree-sitter-markdown',\n",
    "        'tree-sitter-ruby',\n",
    "        'tree-sitter-cpp',\n",
    "        # 'tree-sitter-xml/xml',\n",
    "        'tree-sitter-yaml',\n",
    "        'tree-sitter-go',\n",
    "        'tree-sitter-rust'\n",
    "        # 添加更多语言...\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:25.821088Z",
     "iopub.status.busy": "2025-10-30T02:27:25.820438Z",
     "iopub.status.idle": "2025-10-30T02:27:25.840390Z",
     "shell.execute_reply": "2025-10-30T02:27:25.838824Z",
     "shell.execute_reply.started": "2025-10-30T02:27:25.821031Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载支持的语言库\n",
    "LANGUAGE_MAPPING = {\n",
    "    'python': Language('build/my-languages.so', 'python'),\n",
    "    'java': Language('build/my-languages.so', 'java'),\n",
    "    'javascript': Language('build/my-languages.so', 'javascript'),\n",
    "    'php': Language('build/my-languages.so', 'php'),\n",
    "    'perl': Language('build/my-languages.so', 'perl'),\n",
    "    'c': Language('build/my-languages.so', 'c'),\n",
    "    # 'md': Language('build/my-languages.so', 'markdown'),\n",
    "    'rb': Language('build/my-languages.so', 'ruby'),\n",
    "    'cpp': Language('build/my-languages.so', 'cpp'),\n",
    "    # 'xml': Language('build/my-languages.so', 'xml'),\n",
    "    'yml': Language('build/my-languages.so', 'yaml'),\n",
    "    'go': Language('build/my-languages.so', 'go'),\n",
    "    'rst': Language('build/my-languages.so', 'rust'),\n",
    "    # 可以继续扩展更多语言\n",
    "}\n",
    "\n",
    "def get_language_by_extension(file_path):\n",
    "    if file_path.endswith('.py'):\n",
    "        return 'python'\n",
    "    elif file_path.endswith('.java'):\n",
    "        return 'java'\n",
    "    elif file_path.endswith('.js'):\n",
    "        return 'javascript'\n",
    "    elif file_path.endswith('.php'):\n",
    "        return 'php'\n",
    "    elif file_path.endswith('.cgi'):\n",
    "        return 'perl'\n",
    "    elif file_path.endswith('.c'):\n",
    "        return 'c'\n",
    "    # elif file_path.endswith('.md'):\n",
    "    #     return 'md'\n",
    "    elif file_path.endswith('.rb'):\n",
    "        return 'rb'\n",
    "    elif file_path.endswith('.cpp'):\n",
    "        return 'cpp'\n",
    "    # elif file_path.endswith('.xml'):\n",
    "    #     return 'xml'\n",
    "    elif file_path.endswith('.yml'):\n",
    "        return 'yml'\n",
    "    elif file_path.endswith('.go'):\n",
    "        return 'go'\n",
    "    # elif file_path.endswith('.rst'):\n",
    "    #     return 'rst'\n",
    "    else:\n",
    "        return None  # 不支持的语言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:28.134420Z",
     "iopub.status.busy": "2025-10-30T02:27:28.134050Z",
     "iopub.status.idle": "2025-10-30T02:27:28.149998Z",
     "shell.execute_reply": "2025-10-30T02:27:28.149161Z",
     "shell.execute_reply.started": "2025-10-30T02:27:28.134391Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_ast_tree(code: str, parser: Parser, language: Any) -> Dict:\n",
    "    \"\"\"构建单个代码文件的AST树结构\"\"\"\n",
    "    if not code:\n",
    "        return None\n",
    "    \n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "    \n",
    "    def traverse(node: Node) -> Dict:\n",
    "        \"\"\"递归遍历AST节点\"\"\"\n",
    "        node_dict = {\n",
    "            'type': node.type,\n",
    "            'text': node.text.decode('utf8') if node.text else '',\n",
    "            'children': []\n",
    "        }\n",
    "        \n",
    "        for child in node.children:\n",
    "            if child.is_named:  # 只包含有名称的节点\n",
    "                node_dict['children'].append(traverse(child))\n",
    "                \n",
    "        return node_dict\n",
    "    \n",
    "    return traverse(root_node) if root_node else None\n",
    "\n",
    "def extract_commit_level_ast(diff_text: str) -> Dict:\n",
    "    \"\"\"提取commit级别的AST信息\"\"\"\n",
    "    if not isinstance(diff_text, str) or not diff_text.strip():\n",
    "        return {'commit': {'files': {}, 'ast_before': {}, 'ast_after': {}}}\n",
    "    \n",
    "    try:\n",
    "        patch = PatchSet(io.StringIO(diff_text))\n",
    "        # print('===========',patch)\n",
    "        # print('diff_text',diff_text)\n",
    "    except UnidiffParseError:\n",
    "        \n",
    "        return {'commit': {'files': {}, 'ast_before': {}, 'ast_after': {}}}\n",
    "        \n",
    "    commit_ast = {\n",
    "        'files': {},\n",
    "        'ast_before': {},\n",
    "        'ast_after': {}\n",
    "    }\n",
    "\n",
    "    for file in patch:\n",
    "        file_path = file.path\n",
    "        # print(\"sad asd as sa\",file_path)\n",
    "        lang_key = get_language_by_extension(file_path)\n",
    "\n",
    "        if not lang_key or lang_key not in LANGUAGE_MAPPING:\n",
    "            continue\n",
    "\n",
    "        parser = Parser()\n",
    "        parser.set_language(LANGUAGE_MAPPING[lang_key])\n",
    "\n",
    "        # 收集修改前后的代码\n",
    "        code_before_lines = []\n",
    "        code_after_lines = []\n",
    "        \n",
    "        for hunk in file:\n",
    "            for line in hunk:\n",
    "                if line.is_added:\n",
    "                    code_after_lines.append(line.value)\n",
    "                elif line.is_removed:\n",
    "                    code_before_lines.append(line.value)\n",
    "                else:  # 上下文行\n",
    "                    code_before_lines.append(line.value)\n",
    "                    code_after_lines.append(line.value)\n",
    "\n",
    "        code_before = '\\n'.join(code_before_lines) if code_before_lines else None\n",
    "        code_after = '\\n'.join(code_after_lines) if code_after_lines else None\n",
    "        # print(\"=============\",code_before,code_after)\n",
    "        # 构建AST树\n",
    "        ast_before = build_ast_tree(code_before, parser, LANGUAGE_MAPPING[lang_key])\n",
    "        ast_after = build_ast_tree(code_after, parser, LANGUAGE_MAPPING[lang_key])\n",
    "\n",
    "        # 存储文件信息\n",
    "        commit_ast['files'][file_path] = {\n",
    "            'language': lang_key,\n",
    "            'code_before': code_before,\n",
    "            'code_after': code_after\n",
    "        }\n",
    "\n",
    "        # 存储AST信息\n",
    "        if ast_before:\n",
    "            commit_ast['ast_before'][file_path] = ast_before\n",
    "        if ast_after:\n",
    "            commit_ast['ast_after'][file_path] = ast_after\n",
    "\n",
    "    return {'commit': commit_ast}\n",
    "\n",
    "def extract_commit_level_ast_for_df(diff_text: str) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"为DataFrame设计的commit级别AST提取函数，返回两列数据\"\"\"\n",
    "    result = extract_commit_level_ast(diff_text)\n",
    "    commit_data = result.get('commit', {})\n",
    "    return commit_data.get('ast_before', {}), commit_data.get('ast_after', {})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:28.488977Z",
     "iopub.status.busy": "2025-10-30T02:27:28.488616Z",
     "iopub.status.idle": "2025-10-30T02:27:28.492358Z",
     "shell.execute_reply": "2025-10-30T02:27:28.491674Z",
     "shell.execute_reply.started": "2025-10-30T02:27:28.488949Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['graph_before'], df['graph_after'] = zip(*df['diff'].map(extract_commit_level_ast_for_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:27:28.667248Z",
     "iopub.status.busy": "2025-10-30T02:27:28.666572Z",
     "iopub.status.idle": "2025-10-30T02:29:30.963478Z",
     "shell.execute_reply": "2025-10-30T02:29:30.962720Z",
     "shell.execute_reply.started": "2025-10-30T02:27:28.667190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532959504eb54bada96c15a086bfb63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1264), Label(value='0 / 1264'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['graph_before','graph_after']] = df['diff'].parallel_apply(lambda x: pd.Series(extract_commit_level_ast_for_df(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:29:30.964886Z",
     "iopub.status.busy": "2025-10-30T02:29:30.964619Z",
     "iopub.status.idle": "2025-10-30T02:29:31.209546Z",
     "shell.execute_reply": "2025-10-30T02:29:31.208428Z",
     "shell.execute_reply.started": "2025-10-30T02:29:30.964859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>github</th>\n",
       "      <th>message</th>\n",
       "      <th>diff</th>\n",
       "      <th>label</th>\n",
       "      <th>project_name</th>\n",
       "      <th>graph_before</th>\n",
       "      <th>graph_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/osclass/OSClass/commit/ff7e...</td>\n",
       "      <td>OSClass 2.3.5</td>\n",
       "      <td>diff --git a/CHANGELOG.txt b/CHANGELOG.txt\\nin...</td>\n",
       "      <td>1</td>\n",
       "      <td>osclass</td>\n",
       "      <td>{'oc-admin/ajax/ajax.php': {'type': 'program',...</td>\n",
       "      <td>{'oc-admin/ajax/ajax.php': {'type': 'program',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/gosa-project/gosa-core/comm...</td>\n",
       "      <td>escape html entities to fix xss at the login s...</td>\n",
       "      <td>diff --git a/html/index.php b/html/index.php\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>gosa-project</td>\n",
       "      <td>{'html/index.php': {'type': 'program', 'text':...</td>\n",
       "      <td>{'html/index.php': {'type': 'program', 'text':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/dgl/cgiirc/commit/dd8d50752...</td>\n",
       "      <td>0.5.11</td>\n",
       "      <td>diff --git a/client-perl.cgi b/client-perl.cgi...</td>\n",
       "      <td>0</td>\n",
       "      <td>dgl</td>\n",
       "      <td>{'client-perl.cgi': {'type': 'source_file', 't...</td>\n",
       "      <td>{'client-perl.cgi': {'type': 'source_file', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/GNOME/libxml2/commit/b215c2...</td>\n",
       "      <td>Fix cleanup of attributes in XML reader\\n\\nxml...</td>\n",
       "      <td>diff --git a/xmlreader.c b/xmlreader.c\\nindex ...</td>\n",
       "      <td>0</td>\n",
       "      <td>GNOME</td>\n",
       "      <td>{'xmlreader.c': {'type': 'translation_unit', '...</td>\n",
       "      <td>{'xmlreader.c': {'type': 'translation_unit', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/apache/camel/commit/6b979d0...</td>\n",
       "      <td>CAMEL-10575: snakeyaml: add an option to filte...</td>\n",
       "      <td>diff --git a/components/camel-snakeyaml/src/ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>apache</td>\n",
       "      <td>{'components/camel-snakeyaml/src/main/java/org...</td>\n",
       "      <td>{'components/camel-snakeyaml/src/main/java/org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10107</th>\n",
       "      <td>https://github.com/viabtc/viabtc_exchange_serv...</td>\n",
       "      <td>Update README.md</td>\n",
       "      <td>diff --git a/README.md b/README.md\\nindex 52f6...</td>\n",
       "      <td>0</td>\n",
       "      <td>viabtc</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>https://github.com/qinxuye/cola/commit/d791a1c...</td>\n",
       "      <td>fix counter error; cluster/stage will check st...</td>\n",
       "      <td>diff --git a/cola/cluster/master.py b/cola/clu...</td>\n",
       "      <td>0</td>\n",
       "      <td>qinxuye</td>\n",
       "      <td>{'cola/cluster/master.py': {'type': 'module', ...</td>\n",
       "      <td>{'cola/cluster/master.py': {'type': 'module', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>https://github.com/open-classifieds/openclassi...</td>\n",
       "      <td>Merge pull request #3146 from oliverds/master\\...</td>\n",
       "      <td>diff --git a/oc/classes/image.php b/oc/classes...</td>\n",
       "      <td>0</td>\n",
       "      <td>open-classifieds</td>\n",
       "      <td>{'oc/classes/image.php': {'type': 'program', '...</td>\n",
       "      <td>{'oc/classes/image.php': {'type': 'program', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10110</th>\n",
       "      <td>https://github.com/opnsense/core/commit/2573b7...</td>\n",
       "      <td>firmware: do not show subscription key on firm...</td>\n",
       "      <td>diff --git a/src/opnsense/scripts/firmware/pro...</td>\n",
       "      <td>0</td>\n",
       "      <td>opnsense</td>\n",
       "      <td>{'src/opnsense/scripts/firmware/product.php': ...</td>\n",
       "      <td>{'src/opnsense/scripts/firmware/product.php': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>https://github.com/FFmpeg/FFmpeg/commit/00e818...</td>\n",
       "      <td>avcodec/ac3_parser: Check init_get_bits8() for...</td>\n",
       "      <td>diff --git a/libavcodec/ac3_parser.c b/libavco...</td>\n",
       "      <td>1</td>\n",
       "      <td>FFmpeg</td>\n",
       "      <td>{'libavcodec/ac3_parser.c': {'type': 'translat...</td>\n",
       "      <td>{'libavcodec/ac3_parser.c': {'type': 'translat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10112 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  github  \\\n",
       "0      https://github.com/osclass/OSClass/commit/ff7e...   \n",
       "1      https://github.com/gosa-project/gosa-core/comm...   \n",
       "2      https://github.com/dgl/cgiirc/commit/dd8d50752...   \n",
       "3      https://github.com/GNOME/libxml2/commit/b215c2...   \n",
       "4      https://github.com/apache/camel/commit/6b979d0...   \n",
       "...                                                  ...   \n",
       "10107  https://github.com/viabtc/viabtc_exchange_serv...   \n",
       "10108  https://github.com/qinxuye/cola/commit/d791a1c...   \n",
       "10109  https://github.com/open-classifieds/openclassi...   \n",
       "10110  https://github.com/opnsense/core/commit/2573b7...   \n",
       "10111  https://github.com/FFmpeg/FFmpeg/commit/00e818...   \n",
       "\n",
       "                                                 message  \\\n",
       "0                                          OSClass 2.3.5   \n",
       "1      escape html entities to fix xss at the login s...   \n",
       "2                                                 0.5.11   \n",
       "3      Fix cleanup of attributes in XML reader\\n\\nxml...   \n",
       "4      CAMEL-10575: snakeyaml: add an option to filte...   \n",
       "...                                                  ...   \n",
       "10107                                   Update README.md   \n",
       "10108  fix counter error; cluster/stage will check st...   \n",
       "10109  Merge pull request #3146 from oliverds/master\\...   \n",
       "10110  firmware: do not show subscription key on firm...   \n",
       "10111  avcodec/ac3_parser: Check init_get_bits8() for...   \n",
       "\n",
       "                                                    diff  label  \\\n",
       "0      diff --git a/CHANGELOG.txt b/CHANGELOG.txt\\nin...      1   \n",
       "1      diff --git a/html/index.php b/html/index.php\\n...      1   \n",
       "2      diff --git a/client-perl.cgi b/client-perl.cgi...      0   \n",
       "3      diff --git a/xmlreader.c b/xmlreader.c\\nindex ...      0   \n",
       "4      diff --git a/components/camel-snakeyaml/src/ma...      1   \n",
       "...                                                  ...    ...   \n",
       "10107  diff --git a/README.md b/README.md\\nindex 52f6...      0   \n",
       "10108  diff --git a/cola/cluster/master.py b/cola/clu...      0   \n",
       "10109  diff --git a/oc/classes/image.php b/oc/classes...      0   \n",
       "10110  diff --git a/src/opnsense/scripts/firmware/pro...      0   \n",
       "10111  diff --git a/libavcodec/ac3_parser.c b/libavco...      1   \n",
       "\n",
       "           project_name                                       graph_before  \\\n",
       "0               osclass  {'oc-admin/ajax/ajax.php': {'type': 'program',...   \n",
       "1          gosa-project  {'html/index.php': {'type': 'program', 'text':...   \n",
       "2                   dgl  {'client-perl.cgi': {'type': 'source_file', 't...   \n",
       "3                 GNOME  {'xmlreader.c': {'type': 'translation_unit', '...   \n",
       "4                apache  {'components/camel-snakeyaml/src/main/java/org...   \n",
       "...                 ...                                                ...   \n",
       "10107            viabtc                                                 {}   \n",
       "10108           qinxuye  {'cola/cluster/master.py': {'type': 'module', ...   \n",
       "10109  open-classifieds  {'oc/classes/image.php': {'type': 'program', '...   \n",
       "10110          opnsense  {'src/opnsense/scripts/firmware/product.php': ...   \n",
       "10111            FFmpeg  {'libavcodec/ac3_parser.c': {'type': 'translat...   \n",
       "\n",
       "                                             graph_after  \n",
       "0      {'oc-admin/ajax/ajax.php': {'type': 'program',...  \n",
       "1      {'html/index.php': {'type': 'program', 'text':...  \n",
       "2      {'client-perl.cgi': {'type': 'source_file', 't...  \n",
       "3      {'xmlreader.c': {'type': 'translation_unit', '...  \n",
       "4      {'components/camel-snakeyaml/src/main/java/org...  \n",
       "...                                                  ...  \n",
       "10107                                                 {}  \n",
       "10108  {'cola/cluster/master.py': {'type': 'module', ...  \n",
       "10109  {'oc/classes/image.php': {'type': 'program', '...  \n",
       "10110  {'src/opnsense/scripts/firmware/product.php': ...  \n",
       "10111  {'libavcodec/ac3_parser.c': {'type': 'translat...  \n",
       "\n",
       "[10112 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:29:31.210278Z",
     "iopub.status.busy": "2025-10-30T02:29:31.210056Z",
     "iopub.status.idle": "2025-10-30T02:29:31.217392Z",
     "shell.execute_reply": "2025-10-30T02:29:31.216419Z",
     "shell.execute_reply.started": "2025-10-30T02:29:31.210255Z"
    }
   },
   "outputs": [],
   "source": [
    "from unidiff import PatchSet, UnidiffParseError\n",
    "import io\n",
    "def parse_single_diff(diff_text):\n",
    "    parsed_diff = {}\n",
    "\n",
    "    if not isinstance(diff_text, str) or not diff_text.strip():\n",
    "        return parsed_diff\n",
    "\n",
    "    try:\n",
    "        patch = PatchSet(io.StringIO(diff_text))\n",
    "    except UnidiffParseError:\n",
    "        # print(\"Skipping invalid diff:\", diff_text[:200])  # 仅打印前200字符\n",
    "        return parsed_diff\n",
    "\n",
    "    for file in patch:\n",
    "        file_path = file.path\n",
    "        added_lines = []\n",
    "        removed_lines = []\n",
    "\n",
    "        for hunk in file:\n",
    "            for line in hunk:\n",
    "                if line.is_added:\n",
    "                    added_lines.append(line.value.strip())\n",
    "                elif line.is_removed:\n",
    "                    removed_lines.append(line.value.strip())\n",
    "\n",
    "        if file_path not in parsed_diff:\n",
    "            parsed_diff[file_path] = {'added': [], 'removed': []}\n",
    "        parsed_diff[file_path]['added'].extend(added_lines)\n",
    "        parsed_diff[file_path]['removed'].extend(removed_lines)\n",
    "\n",
    "    return parsed_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:29:31.219001Z",
     "iopub.status.busy": "2025-10-30T02:29:31.218753Z",
     "iopub.status.idle": "2025-10-30T02:30:54.475171Z",
     "shell.execute_reply": "2025-10-30T02:30:54.474285Z",
     "shell.execute_reply.started": "2025-10-30T02:29:31.218977Z"
    }
   },
   "outputs": [],
   "source": [
    "df['code_diff'] = df['diff'].map(lambda  x : parse_single_diff(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:58:29.919697Z",
     "iopub.status.busy": "2025-10-30T02:58:29.919392Z",
     "iopub.status.idle": "2025-10-30T02:58:29.942172Z",
     "shell.execute_reply": "2025-10-30T02:58:29.940978Z",
     "shell.execute_reply.started": "2025-10-30T02:58:29.919672Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn.dense.diff_pool import dense_diff_pool\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from typing import Dict, List\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASTGraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.node_counter = 0\n",
    "        self.nodes = []\n",
    "        self.edges = []\n",
    "        self.node_type_map = {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.node_counter = 0\n",
    "        self.nodes = []\n",
    "        self.edges = []\n",
    "        self.node_type_map = {}\n",
    "\n",
    "    def traverse_ast_dict(self, ast_dict: Dict, parent_idx: int = None):\n",
    "        \"\"\"递归遍历AST字典结构构建图\"\"\"\n",
    "        if ast_dict is None:\n",
    "            return\n",
    "        \n",
    "        # 处理当前节点\n",
    "        current_idx = self.node_counter\n",
    "        self.node_counter += 1\n",
    "        \n",
    "        node_type = ast_dict['type']\n",
    "        if node_type not in self.node_type_map:\n",
    "            self.node_type_map[node_type] = len(self.node_type_map)\n",
    "        \n",
    "        self.nodes.append(node_type)\n",
    "        \n",
    "        # 添加与父节点的边\n",
    "        if parent_idx is not None:\n",
    "            self.edges.append((parent_idx, current_idx))\n",
    "        \n",
    "        # 递归处理子节点\n",
    "        for child in ast_dict.get('children', []):\n",
    "            self.traverse_ast_dict(child, current_idx)\n",
    "\n",
    "def dict_ast_to_graph(ast_dict: Dict) -> Data:\n",
    "    \"\"\"将字典形式的AST转换为图数据\"\"\"\n",
    "    builder = ASTGraphBuilder()\n",
    "    builder.traverse_ast_dict(ast_dict)\n",
    "    \n",
    "    if not builder.nodes:\n",
    "        return Data(x=torch.zeros((1, 1)), edge_index=torch.zeros((2, 0), dtype=torch.long))\n",
    "    \n",
    "    # 创建节点特征和边索引\n",
    "    x = torch.tensor([builder.node_type_map[t] for t in builder.nodes], dtype=torch.long)\n",
    "    edge_index = torch.tensor(builder.edges, dtype=torch.long).t().contiguous() if builder.edges else torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "class CommitDiffPoolDense(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, assign_dim, out_dim, max_nodes):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(in_dim, hidden_dim)\n",
    "        self.gnn_embed = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.gnn_assign = GCNConv(hidden_dim, assign_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim)\n",
    "        self.max_nodes = max_nodes  # 用于 dense 变换\n",
    "\n",
    "    def forward_single_graph(self, data: Data):\n",
    "        \"\"\"处理单个AST图 (DiffPool Dense 版本，自动补齐维度)\"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        num_nodes = x.size(0)\n",
    "    \n",
    "        # 节点嵌入\n",
    "        x = self.emb(x).squeeze(1)  # [N, hidden_dim]\n",
    "    \n",
    "        # 分支1: 节点特征编码\n",
    "        z = F.relu(self.gnn_embed(x, edge_index))  # [N, hidden_dim]\n",
    "        # 分支2: 分配矩阵\n",
    "        s = F.softmax(self.gnn_assign(x, edge_index), dim=-1)  # [N, assign_dim]\n",
    "    \n",
    "        # === Dense 变换 ===\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=self.max_nodes)[0]  # [N_max, N_max]\n",
    "        # pad 特征到 max_nodes\n",
    "        num_nodes = x.size(0)\n",
    "        max_nodes = self.max_nodes\n",
    "        \n",
    "        # --- 防止节点数溢出 ---\n",
    "        if num_nodes > max_nodes:\n",
    "            z = z[:max_nodes]\n",
    "            s = s[:max_nodes]\n",
    "            num_nodes = max_nodes\n",
    "        \n",
    "        # pad 特征到 max_nodes\n",
    "        z_padded = torch.zeros(max_nodes, z.size(1), device=z.device)\n",
    "        s_padded = torch.zeros(max_nodes, s.size(1), device=s.device)\n",
    "        z_padded[:num_nodes] = z\n",
    "        s_padded[:num_nodes] = s\n",
    "        \n",
    "        # 增加 batch 维度\n",
    "        z_padded = z_padded.unsqueeze(0)\n",
    "        s_padded = s_padded.unsqueeze(0)\n",
    "        adj = adj.unsqueeze(0)\n",
    "    \n",
    "        # === DiffPool ===\n",
    "        out, out_adj, link_loss, ent_loss = dense_diff_pool(\n",
    "            z_padded, adj, s_padded, mask=None, normalize=True\n",
    "        )\n",
    "    \n",
    "        # out: [1, assign_dim, hidden_dim]\n",
    "        graph_emb = out.mean(dim=1).squeeze(0)  # [hidden_dim]\n",
    "        logits = self.linear(graph_emb)\n",
    "        return logits\n",
    "\n",
    "    def forward_file_ast(self, ast_dict: Dict):\n",
    "        graph = dict_ast_to_graph(ast_dict)\n",
    "        return self.forward_single_graph(graph)\n",
    "\n",
    "    def forward_commit(self, ast_before: Dict, ast_after: Dict):\n",
    "        file_embeddings_before = []\n",
    "        file_embeddings_after = []\n",
    "        all_files = set(ast_before.keys()).union(set(ast_after.keys()))\n",
    "        for file_path in all_files:\n",
    "            if file_path in ast_before and ast_before[file_path]:\n",
    "                emb_b = self.forward_file_ast(ast_before[file_path])\n",
    "                if emb_b.dim() == 1:\n",
    "                    emb_b = emb_b.unsqueeze(0)\n",
    "                file_embeddings_before.append(emb_b)\n",
    "            else:\n",
    "                file_embeddings_before.append(torch.zeros(1, self.linear.out_features))\n",
    "            if file_path in ast_after and ast_after[file_path]:\n",
    "                emb_a = self.forward_file_ast(ast_after[file_path])\n",
    "                if emb_a.dim() == 1:\n",
    "                    emb_a = emb_a.unsqueeze(0)\n",
    "                file_embeddings_after.append(emb_a)\n",
    "            else:\n",
    "                file_embeddings_after.append(torch.zeros(1, self.linear.out_features))\n",
    "        if file_embeddings_before:\n",
    "            h_before = torch.cat(file_embeddings_before, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            h_before = torch.zeros(self.linear.out_features)\n",
    "        if file_embeddings_after:\n",
    "            h_after = torch.cat(file_embeddings_after, dim=0).mean(dim=0)\n",
    "        else:\n",
    "            h_after = torch.zeros(self.linear.out_features)\n",
    "        diff_embedding = torch.cat([h_before, h_after - h_before], dim=-1).detach()\n",
    "        return diff_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T02:59:06.331259Z",
     "iopub.status.busy": "2025-10-30T02:59:06.330749Z",
     "iopub.status.idle": "2025-10-30T03:02:12.952619Z",
     "shell.execute_reply": "2025-10-30T03:02:12.951485Z",
     "shell.execute_reply.started": "2025-10-30T02:59:06.331217Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1️⃣ 模型初始化\n",
    "model = CommitDiffPoolDense(\n",
    "    in_dim=128,        # 节点词典大小（视你的AST节点id范围而定）\n",
    "    hidden_dim=64,     # GCN隐藏层维度\n",
    "    assign_dim=32,     # DiffPool分配矩阵的子图数，可设为 hidden_dim // 2\n",
    "    out_dim=32,        # 最终图嵌入维度\n",
    "    max_nodes=200      # 每个AST的最大节点数（超过会截断）\n",
    ")\n",
    "df['embedding'] = df.apply(lambda row: model.forward_commit(row['graph_before'], row['graph_after']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:12.954049Z",
     "iopub.status.busy": "2025-10-30T03:02:12.953827Z",
     "iopub.status.idle": "2025-10-30T03:02:12.959405Z",
     "shell.execute_reply": "2025-10-30T03:02:12.958441Z",
     "shell.execute_reply.started": "2025-10-30T03:02:12.954029Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def to_numpy_vector(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.numpy()\n",
    "    elif isinstance(x, list) and isinstance(x[0], torch.Tensor):\n",
    "        return torch.stack(x).numpy()\n",
    "    elif isinstance(x, list) and isinstance(x[0], float):\n",
    "        return np.array(x)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(x)} -> {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:12.960119Z",
     "iopub.status.busy": "2025-10-30T03:02:12.959925Z",
     "iopub.status.idle": "2025-10-30T03:02:13.052011Z",
     "shell.execute_reply": "2025-10-30T03:02:13.051533Z",
     "shell.execute_reply.started": "2025-10-30T03:02:12.960102Z"
    }
   },
   "outputs": [],
   "source": [
    "# 先转换为 numpy 向量\n",
    "df['embedding_np'] = df['embedding'].apply(to_numpy_vector)\n",
    "\n",
    "# 拼成矩阵\n",
    "X = np.stack(df['embedding_np'].values)\n",
    "\n",
    "# 聚类\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "distances = kmeans.fit_transform(X)  # 获取每个点到各聚类中心的距离\n",
    "\n",
    "# 将距离转换为概率（使用softmax）\n",
    "soft_labels = np.exp(-distances) / np.sum(np.exp(-distances), axis=1, keepdims=True)\n",
    "df['soft_labels'] = list(soft_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:13.053273Z",
     "iopub.status.busy": "2025-10-30T03:02:13.053065Z",
     "iopub.status.idle": "2025-10-30T03:02:13.055879Z",
     "shell.execute_reply": "2025-10-30T03:02:13.055184Z",
     "shell.execute_reply.started": "2025-10-30T03:02:13.053251Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['soft_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T03:02:13.056615Z",
     "iopub.status.busy": "2025-10-30T03:02:13.056428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages shape: (10112,)\n",
      "cluster_vectors shape: (10112,)\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6847373873462b8e611119af7505b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4029 | Train Acc: 0.8190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f22ea7591b4eee8b587b869b0751c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2873 | Val Acc: 0.8767\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce4aed360264d59b87153981ee54c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. 定义数据集类\n",
    "class CommitDataset(Dataset):\n",
    "    def __init__(self, messages, cluster_labels, labels, diffs, tokenizer, diff_tokenizer, max_length=128):\n",
    "        self.messages = messages\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.labels = labels\n",
    "        self.diffs = diffs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.diff_tokenizer = diff_tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.messages)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        message = str(self.messages[idx])\n",
    "        cluster_labels = self.cluster_labels[idx]\n",
    "        label = self.labels[idx]\n",
    "        diff = str(self.diffs[idx])\n",
    "        \n",
    "        # Tokenize the commit message\n",
    "        message_encoding = self.tokenizer(\n",
    "            message,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize the diff using the diff_tokenizer (CodeBERT)\n",
    "        diff_encoding = self.diff_tokenizer(\n",
    "            diff,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': message_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': message_encoding['attention_mask'].flatten(),\n",
    "            'diff_input_ids': diff_encoding['input_ids'].flatten(),\n",
    "            'diff_attention_mask': diff_encoding['attention_mask'].flatten(),\n",
    "            'cluster_onehot': torch.tensor(cluster_labels, dtype=torch.float),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "# 2. 定义模型\n",
    "# CNN\n",
    "\n",
    "class CommitClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, codebert_model_name, num_classes, \n",
    "                 cluster_dim=10, hidden_dim=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        带聚类增强的提交分类模型\n",
    "        \n",
    "        参数:\n",
    "            bert_model_name: 用于提交信息的BERT模型名称\n",
    "            codebert_model_name: 用于代码差异的CodeBERT模型名称\n",
    "            num_classes: 分类类别数\n",
    "            cluster_dim: 聚类标签的维度(one-hot长度)\n",
    "            hidden_dim: 隐藏层维度\n",
    "            dropout: dropout率\n",
    "        \"\"\"\n",
    "        super(CommitClassifier, self).__init__()\n",
    "        \n",
    "        # 文本(提交信息)编码器\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # 代码(差异)编码器\n",
    "        self.codebert = AutoModel.from_pretrained(codebert_model_name)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 获取各模态的隐藏维度\n",
    "        bert_hidden_dim = self.bert.config.hidden_size\n",
    "        codebert_hidden_dim = self.codebert.config.hidden_size\n",
    "        \n",
    "        # 投影层对齐维度\n",
    "        self.text_proj = nn.Linear(bert_hidden_dim, hidden_dim)\n",
    "        self.code_proj = nn.Linear(codebert_hidden_dim, hidden_dim)\n",
    "        \n",
    "        # 聚类标签处理层 - 保持原始维度不压缩\n",
    "        self.cluster_proj = nn.Linear(cluster_dim, hidden_dim) if cluster_dim > 0 else None\n",
    "        \n",
    "        # 计算分类器输入维度\n",
    "        classifier_input_dim = 2 * hidden_dim  # 文本 + 代码\n",
    "        if cluster_dim > 0:\n",
    "            classifier_input_dim += hidden_dim  # 加上聚类特征(不再压缩)\n",
    "            \n",
    "        # 最终分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, diff_input_ids, diff_attention_mask, cluster_labels=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "            input_ids: 提交信息的token IDs\n",
    "            attention_mask: 提交信息的attention mask\n",
    "            diff_input_ids: 代码差异的token IDs\n",
    "            diff_attention_mask: 代码差异的attention mask\n",
    "            cluster_labels: 聚类标签的one-hot编码 (batch_size, cluster_dim)\n",
    "        \"\"\"\n",
    "        # 处理提交信息\n",
    "        text_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        text_pooled = text_output.pooler_output\n",
    "        text_features = torch.relu(self.text_proj(text_pooled))\n",
    "        \n",
    "        # 处理代码差异\n",
    "        code_output = self.codebert(\n",
    "            input_ids=diff_input_ids,\n",
    "            attention_mask=diff_attention_mask\n",
    "        )\n",
    "        code_pooled = code_output.pooler_output\n",
    "        code_features = torch.relu(self.code_proj(code_pooled))\n",
    "        \n",
    "        # 处理聚类标签(如果提供)\n",
    "        if cluster_labels is not None and self.cluster_proj is not None:\n",
    "            cluster_features = torch.relu(self.cluster_proj(cluster_labels.float()))\n",
    "        else:\n",
    "            cluster_features = None\n",
    "        \n",
    "        # 组合所有特征\n",
    "        if cluster_features is not None:\n",
    "            combined = torch.cat((text_features, code_features, cluster_features), dim=1)\n",
    "        else:\n",
    "            combined = torch.cat((text_features, code_features), dim=1)\n",
    "        \n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # 分类\n",
    "        logits = self.classifier(combined)\n",
    "        return logits, combined\n",
    "\n",
    "\n",
    "# EarlyStopping类保持不变\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, learning_rate=2e-5, patience=3):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.best_model_path = 'best_model.pt'\n",
    "        self.early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            diff_input_ids = batch['diff_input_ids'].to(self.device)\n",
    "            diff_attention_mask = batch['diff_attention_mask'].to(self.device)\n",
    "            cluster_labels = batch['cluster_onehot'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs, _ = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                diff_input_ids=diff_input_ids,\n",
    "                diff_attention_mask=diff_attention_mask,\n",
    "                cluster_labels=cluster_labels\n",
    "            )\n",
    "            \n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = total_loss / len(self.train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        return train_loss, train_acc\n",
    "\n",
    "    # eval_model函数保持不变\n",
    "    def eval_model(self,dataloader):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                diff_input_ids = batch['diff_input_ids'].to(self.device)\n",
    "                diff_attention_mask = batch['diff_attention_mask'].to(self.device)\n",
    "                cluster_labels = batch['cluster_onehot'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs, embeddings = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    diff_input_ids=diff_input_ids,\n",
    "                    diff_attention_mask=diff_attention_mask,\n",
    "                    cluster_labels=cluster_labels\n",
    "                )\n",
    "                \n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_embeddings.extend(embeddings.cpu().numpy())\n",
    "        \n",
    "        return total_loss / len(dataloader), correct / total, all_preds, all_labels, all_embeddings\n",
    "    \n",
    "    def validate(self, dataloader=None):\n",
    "        if dataloader is None:\n",
    "            dataloader = self.val_loader\n",
    "            \n",
    "        return self.eval_model(dataloader)\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            # 训练一个epoch\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "            \n",
    "            # 验证\n",
    "            val_loss, val_acc, _, _,_ = self.validate()\n",
    "            print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # 早停检查\n",
    "            if self.early_stopper.early_stop(val_loss):             \n",
    "                break\n",
    "        \n",
    "        # 训练结束后加载最佳模型\n",
    "        # self.model.load_state_dict(torch.load(self.best_model_path))\n",
    "    \n",
    "    def evaluate(self, test_loader):\n",
    "        _, test_acc, test_preds, test_labels, test_embeddings = self.validate(test_loader)\n",
    "        return test_acc, test_preds, test_labels, test_embeddings\n",
    "\n",
    "    def plot_tsne(self, test_loader):\n",
    "        labels = ['positive', 'negative']\n",
    "        _, test_acc, test_preds, test_labels, test_embeddings = self.validate(test_loader)\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "    \n",
    "        # embeddings_np = test_embeddings  # Fixed this line\n",
    "        embeddings_np = np.vstack(test_embeddings) \n",
    "        n_samples = embeddings_np.shape[0]\n",
    "        # print(\"=======================\",n_samples)\n",
    "        # print(test_embeddings)\n",
    "        # Apply t-SNE transformation\n",
    "        embeddings_2d = tsne.fit_transform(embeddings_np)  # Changed to use embeddings_np\n",
    "        df_tsne = pd.DataFrame(embeddings_2d, columns=['TSNE1', 'TSNE2'])\n",
    "        \n",
    "        # Apply KMeans\n",
    "        kmeans_model = KMeans(n_clusters=2, random_state=42, n_init='auto').fit(embeddings_np)\n",
    "        cluster_labels = kmeans_model.labels_  # Changed from fit_predict to labels_\n",
    "        \n",
    "        df_tsne['Cluster'] = cluster_labels\n",
    "        df_tsne['TrueLabel'] = test_labels  # Added true labels for comparison\n",
    "\n",
    "\n",
    "        # Silhouette Score\n",
    "        sil_score = silhouette_score(embeddings_np, cluster_labels)\n",
    "        print(\"Silhouette Score:\", sil_score)\n",
    "\n",
    "        # Plot using `plt.plot`\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for cluster_id in df_tsne['Cluster'].unique():\n",
    "            cluster_data = df_tsne[df_tsne['Cluster'] == cluster_id]\n",
    "            plt.scatter(cluster_data['TSNE1'], cluster_data['TSNE2'], \n",
    "                       label=f'Cluster {cluster_id}')\n",
    "        \n",
    "        plt.title('Scatter plot of embeddings using KMeans Clustering')\n",
    "        plt.xlabel('TSNE1')\n",
    "        plt.ylabel('TSNE2')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.show()  # Changed from pause/clf to show for better visualizationlabels\n",
    "\n",
    "        # 计算每个样本的 Silhouette 系数\n",
    "        sample_silhouette_values = silhouette_samples(embeddings_np, cluster_labels)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        y_lower = 10\n",
    "        for i in range(2):  # 2 是聚类个数\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "            \n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            \n",
    "            color = cm.nipy_spectral(float(i) / 2)\n",
    "            plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            \n",
    "            plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            y_lower = y_upper + 10  # 10 for spacing\n",
    "        \n",
    "        plt.axvline(x=sil_score, color=\"red\", linestyle=\"--\")\n",
    "        plt.title(\"Silhouette Plot for KMeans Clustering\")\n",
    "        plt.xlabel(\"Silhouette Coefficient Values\")\n",
    "        plt.ylabel(\"Cluster Label\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, test_loader, normalize=False, title='Confusion Matrix on Dataset I', cmap=plt.cm.Blues):\n",
    "        classes = ['positive','negative']\n",
    "        \n",
    "        _, test_acc, test_preds, test_labels, test_embeddings = self.validate(test_loader)\n",
    "        cm = confusion_matrix(test_labels, test_preds)\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "    \n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "    \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], fmt),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "        plt.ylabel('True label')\n",
    "        # plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# 主函数\n",
    "# def main():\n",
    "# 配置参数\n",
    "BERT_MODEL_NAME = './models/bert-base-uncased'\n",
    "CODEBERT_MODEL_NAME = './models/codebert-base'\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 512\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "messages = df['message'].values\n",
    "diff = df['code_diff'].values\n",
    "cluster_labels = df['soft_labels'].values\n",
    "labels = df['label'].values\n",
    "print(f\"messages shape: {messages.shape}\")\n",
    "print(f\"cluster_vectors shape: {cluster_labels.shape}\")\n",
    "\n",
    "train_messages, temp_messages, train_diff, temp_diff, train_cluster_labels, temp_cluster_labels, train_labels, temp_labels = train_test_split(\n",
    "    messages, diff, cluster_labels, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 第二次划分：val + test（从 temp 中再拆）\n",
    "val_messages, test_messages, val_diff, test_diff, val_cluster_labels, test_cluster_labels, val_labels, test_labels = train_test_split(\n",
    "    temp_messages, temp_diff, temp_cluster_labels, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "# 初始化tokenizer\n",
    "message_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "diff_tokenizer = AutoTokenizer.from_pretrained(CODEBERT_MODEL_NAME)\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = CommitDataset(\n",
    "    messages=train_messages,\n",
    "    cluster_labels=train_cluster_labels,\n",
    "    labels=train_labels,\n",
    "    diffs=train_diff,\n",
    "    tokenizer=message_tokenizer,\n",
    "    diff_tokenizer=diff_tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = CommitDataset(\n",
    "    messages=val_messages,\n",
    "    cluster_labels=val_cluster_labels,\n",
    "    labels=val_labels,\n",
    "    diffs=val_diff,\n",
    "    tokenizer=message_tokenizer,\n",
    "    diff_tokenizer=diff_tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "test_dataset = CommitDataset(\n",
    "    messages=test_messages,\n",
    "    cluster_labels=test_cluster_labels,\n",
    "    labels=test_labels,\n",
    "    diffs=test_diff,\n",
    "    tokenizer=message_tokenizer,\n",
    "    diff_tokenizer=diff_tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 初始化模型和训练器\n",
    "# model = CommitClassifier(\n",
    "#     bert_model_name=BERT_MODEL_NAME,\n",
    "#     codebert_model_name=CODEBERT_MODEL_NAME,\n",
    "#     meta_dim=META_DIM,\n",
    "#     num_classes=NUM_CLASSES\n",
    "# )\n",
    "\n",
    "# 初始化模型\n",
    "model = CommitClassifier(\n",
    "    bert_model_name=BERT_MODEL_NAME,\n",
    "    codebert_model_name=CODEBERT_MODEL_NAME,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    cluster_dim=2,  # 聚类类别数\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train(epochs=EPOCHS)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T04:29:17.581997Z",
     "iopub.status.busy": "2025-10-30T04:29:17.581331Z",
     "iopub.status.idle": "2025-10-30T04:30:49.121094Z",
     "shell.execute_reply": "2025-10-30T04:30:49.120199Z",
     "shell.execute_reply.started": "2025-10-30T04:29:17.581938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cc9b62e65e4b0b8604a672a31679cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9050758075148319\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9130    0.9412    0.9269       970\n",
      "           1     0.8897    0.8410    0.8647       547\n",
      "\n",
      "    accuracy                         0.9051      1517\n",
      "   macro avg     0.9014    0.8911    0.8958      1517\n",
      "weighted avg     0.9046    0.9051    0.9045      1517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "test_acc, test_preds, test_labels, _ = trainer.evaluate(test_loader)\n",
    "\n",
    "print('\\nTest Accuracy:', test_acc)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(test_labels, test_preds,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-30T04:33:25.774Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.plot_tsne(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-30T04:33:25.774Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.plot_confusion_matrix(test_loader,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-30T04:33:25.774Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"TriFusion_model_900repo_nothing.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-30T04:33:25.774Z"
    }
   },
   "outputs": [],
   "source": [
    "#发送多种类型的邮件\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import smtplib\n",
    "\n",
    "from email.mime.text import MIMEText\n",
    "msg_from = '915803745@qq.com'  # 发送方邮箱\n",
    "passwd = 'vcuosuurrgkfbdai'   #就是上面的授权码\n",
    " \n",
    "# to= ['g.zhang@gotion.com', 'j.tong@gotion.com'] #接受方邮箱\n",
    "to= ['j.tong@gotion.com'] #接受方邮箱\n",
    "#设置邮件内容\n",
    "#MIMEMultipart类可以放任何内容\n",
    "msg = MIMEMultipart()\n",
    "conntent=f\"{classification_report(test_labels, test_preds, digits=4)}\"\n",
    "#把内容加进去\n",
    "msg.attach(MIMEText(conntent,'plain','utf-8'))\n",
    " \n",
    "#设置邮件主题\n",
    "msg['Subject']=\"强化学习模型训练完毕\"\n",
    " \n",
    "#发送方信息\n",
    "msg['From']=msg_from\n",
    " \n",
    "#开始发送\n",
    " \n",
    "#通过SSL方式发送，服务器地址和端口\n",
    "s = smtplib.SMTP_SSL(\"smtp.qq.com\", 465)\n",
    "# 登录邮箱\n",
    "s.login(msg_from, passwd)\n",
    "#开始发送\n",
    "s.sendmail(msg_from,to,msg.as_string())\n",
    "print(\"强化学习模型训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-30T02:25:55.736035Z",
     "iopub.status.idle": "2025-10-30T02:25:55.736265Z",
     "shell.execute_reply": "2025-10-30T02:25:55.736154Z",
     "shell.execute_reply.started": "2025-10-30T02:25:55.736142Z"
    }
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
